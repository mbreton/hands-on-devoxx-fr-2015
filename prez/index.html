<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Initiation au Machine Learning avec Spark</title>


    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">

    <link rel="stylesheet" type="text/css" href="css/theme/devoxx.css" id="theme">


    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/solarized_light.css">

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

    <style>
        pre.small-code {
            font-size: 0.36em;
        }
    </style>
</head>

<body>

<div class="reveal center">
    <div class="slides">
        <section class="front-page" data-background-image="images/bg.png">
            <h1>Initiation au Machine Learning avec Spark</h1>
            <aside class="notes">
                <!-- Put our comments her -->
            </aside>
        </section>
        <section>
            <h2>Le Slide égocentrique obligatoire</h2>
            <div>
                <div class="avatar-container">
                    <h6>Alban Phelip</h6>
                    <img src="https://media.licdn.com/mpr/mpr/shrink_200_200/AAEAAQAAAAAAAAFyAAAAJGE0MDBhYTJmLTk3NzMtNDZkOS1iY2FkLWI2Mzk1OTA0NjJhZQ.jpg"
                         class="avatar"/>
                    <p>Data Scientist</p>
                </div>
                <div class="avatar-container">
                    <h6>Yoann Benoit</h6>
                    <img src="https://pbs.twimg.com/profile_images/537182058540318720/BIo6pMtt.jpeg" class="avatar"/>
                    <p>Data Scientist</p>
                </div>
                <div class="avatar-container">
                    <h6>Mathieu Breton</h6>
                    <img src="https://pbs.twimg.com/profile_images/458752749375475712/Rxm6_6Cr.png" class="avatar"/>
                    <p>Full Stack Developper</p>
                    <style>
                        .reveal .avatar-container{
                            float:left;
                            width: 33%;
                            text-align: center;
                            padding-bottom: 10px;
                        }
                        .reveal .avatar-container .avatar{
                            width: 70%;
                            margin:10px;
                            border-radius: 10px;
                            -moz-box-shadow: 5px 5px 20px 0px #cfcfcf;
                            -webkit-box-shadow: 5px 5px 20px 0px #cfcfcf;
                            -o-box-shadow: 5px 5px 20px 0px #cfcfcf;
                            box-shadow: 5px 5px 20px 0px #cfcfcf;
                            filter:progid:DXImageTransform.Microsoft.Shadow(color=#cfcfcf, Direction=134, Strength=20);
                        }
                    </style>
                </div>
                <div style="clear: both;border-top: 1px solid #CC6528;text-align: center">
                    <p>Xebian's team</p>
                </div>
            </div>
            <aside class="notes">
                <!-- Put our comments her -->
            </aside>
        </section>
        <section>
            <h2>Agenda</h2>
            <ol>
                <li>Les bases du Machine Learning</li>
                <li>TP Kmeans</li>
                <li>TP Naive Bayes</li>
                <li>- Break -</li>
                <li>Présentation de Spark & MLlib</li>
                <li>Naive Bayes avec Spark</li>
                <li>Random Forest avec Spark</li>
            </ol>
            <aside class="notes">
                <!-- Put our comments her -->
            </aside>
        </section>
        <section>
            <h2 class="alone">Les bases du machine learning</h2>
            <aside class="notes">
                <!-- Put our comments her -->
            </aside>
        </section>
        <section>
            <section>
                <h3>Qu'est ce donc ?</h3>
                <blockquote>
                    L’apprentissage machine est l’étude des algorithmes
                    qui s’améliorent automatiquement en fonction de l’expérience.
                </blockquote>
                -- Tom Mitchell
                <aside class="notes">
                    <!-- Put our comments her -->
                </aside>
            </section>
            <section>
                On utilise leurs résultats sans forécement s'en rendre compte
                <aside class="notes">
                    <!-- Put our comments her -->
                </aside>
            </section>
        </section>
        <section>
            <h3>Cas d'utilisation</h3>
            <p>Classification</p>
        </section>
        <section>
            <h3>Cas d'utilisation</h3>
            <div>
                <p>Régréssion</p>
            </div>
            <div>
                <p>Graph</p>
            </div>
        </section>
        <section>
            <h3>Cas d'utilisation</h3>
            <div>
                <p>Recommandation</p>
                <img src="images/reco.jpg" >
            </div>
            <div>
                <p>Graph</p>
            </div>
        </section>
        <section>
            <h3>Cas d'utilisation</h3>
            <div>
                <p>Clustering</p>
                <img src="images/reco.jpg" >
            </div>
            <div>
                <p>Clustering</p>
            </div>
        </section>
        <section>
            <h3>Approches : supervisé (dirigé)</h3>
               On dispose d'exemples, et pour chaque exemple, le résultat attendu.
            <p>
                Ex: Des tweets et leurs tags associés
            </p>
        </section>
        <section>
            <h3>Approches : non supervisé</h3>
            On dispose de données brutes, mais pas de résultat attendu. Juste des données brutes.
            <p>
                Ex: Logs server web
            </p>
        </section>
        <section>
            <h3>Approches : apprentissage par renforcement</h3>
            Pour chaque état dans notre domaine, on peut associer une récompense.
            <p>
                Ex: Vol acrobatique d'hélicoptère
            </p>
            <a href="http://cs.stanford.edu/groups/helicopter/papers/nips06-aerobatichelicopter.pdf">cs.stanford.edu/groups/helicopter/papers/nips06-aerobatichelicopter.pdf</a>
        </section>
        <section>
            <h3>Comment en arrive-t'on à avoir besoin de ML ?</h3>
            <ol>
                <li>On observe un phénomène</li>
                <li>On a du mal à trouver un algo précis</li>
                <li>On utilise un algo qui va apprendre à approximer le phénomène réel</li>
            </ol>
        </section>
        <section>
            <h3>A l'utilisation, comment ça se passe ?</h3>
            Le machine Machine learning c'est comme la crypto
            <p>Utilisez des implémentations éprouvées.</p>
        </section>
        <section>
            <h2>Mais à quoi "ça" ressemble ?</h2>
        </section>
        <section>
            <h3>Kmeans</h3>
            <section>
                <p>Type: Non supervisé</p>
                <p>But: Trouver des groupes dans un jeu de données</p>
            </section>
            <section>
                <p>Exemple : Quels groupes puis-je identifier dans ma base client ?</p>
            </section>
            <section>
                <ul>
                    <li>Client 1 : 24ans, 1 exemplaire</li>
                    <li>Client 2 : 32ans, 2 exemplaires</li>
                    <li>Client 3 : 27ans, 5 exemplaires</li>
                </ul>
            </section>
            <section>
                (24;1), (32;2), (37;5)
            </section>
            <section>
                <img src="images/kmean1.png">
            </section>
            <section>
                <img src="images/kmean2.png">
            </section>
            <section>
                <img src="images/kmean3.png">
            </section>
            <section>
                <img src="images/kmean4.png">
            </section>
            <section>
                <img src="images/kmean5.png">
            </section>
            <section>
                <img src="images/kmean6.png">
            </section>
        </section>
        <section>
            <h2>Classification naive Bayesienne</h2>
        </section>
        <section>
            <h3>Classification naive Bayesienne</h3>
            <section>
                <p>Type: Supervisé</p>
                <p>But: Déterminer la catégorie d'un élément</p>
            </section>
            <section>
                <p>Probabilités</p>
                <p>&Omega;, F, P</p>
            </section>
            <section>
                <p>$$p(A|B) = \frac{p(B|A) p(A)}{p(B)}$$</p>
            </section>
            <section>
                <p>M : J'ai un message M</p>
                <p>S : C'est un spam</p>
                <p>$$p(S|M) = \frac{p(M|S) p(S)}{p(M)}$$</p>
            </section>
            <section id="naive-too-long">
                <p>$$p(S|W_1,...,W_n) = \frac{p(W_1,...,W_n|S) p(S)}{p(W_1,...,W_n)}$$</p>
            </section>
            <section>
                <p>$$k=\frac{p(S=1|W_1,...,W_n)}{p(S=0|W_1,...,W_n)}$$</p>
                <p> </p>
                <p>$$= \frac{p(W_1,...,W_n|S=1)* p(S=1)}{p(W_1,...,W_n)}*\frac{p(W_1,...,W_n)}{p(W_1,...,W_n|S=0)* p(S=0)}$$</p>
            </section>
            <section>
                <p>Hypothèse dans le cas naif : indépendance</p>
                $$p(A,B|C)=p(A|C)p(B|C)$$
                $$p(W_1,...W_n|S=1)=p(W_1|S=1) p(W_2|S=1)...(W_n|S=1)$$
            </section>
            <section>
                $$p(W_1,...W_n|S)=\prod_{i=1}^{N}{p(W_i|S)}$$
                $$k=\frac{\prod_{i=1}^{N}{p(W_i|S=1)p(S=1)}}{\prod_{i=1}^{N}{p(W_i|S=0) * p(S=0)}}$$
            </section>
            <section id="naive-4">
                <h4>Du coup ?</h4>
                <p>$$k=\frac{\prod_{i=1}^{N}{p(W_i|S=1)p(S=1)}}{\prod_{i=1}^{N}{p(W_i|S=0) * p(S=0)}}$$</p>
                <p>$$p(S=1)=\frac{\text{nombre de spams observés}}{\text{nombre total de messages}}$$</p>
                <p>$$p(S=0) = 1 - p(S=1)$$</p>
                <p>$$p(W_i|S=t)=\frac{\text{nombre de messages de type t dans lesquels }W_i\text{ apparait}}{\text{nombre de messages de type t}}$$</p>
            </section>
            <section>
                <h2>Implémentation</h2>
            </section>
            <section>
                <h3>Un peu de recul sur l'algo</h3>
                Est-ce un bon score ? (> 0.5 et |spam|/|total| ?)<br/>
                Est-ce que mes données d'entraînement sont suffisament représentatives ?<br/>
                Est-ce que mes résultats se généralisent ?<br/>
                L'approche naïve est elle pertinente ?
                <aside class="notes">
                    données représentatives: est-ce que le ratio nb spam/nb ham est représentatif de ce que l’on retrouve dans la nature ? (vu le point de ce rapport)
                </aside>
            </section>
        </section>
        <section>
            <h2 class="alone">Spark & MLlib</h2>
            <aside class="notes">
                <!-- Put our comments her -->
            </aside>
        </section>
        <section>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161394/logo_hadoop.png">
        </section>
        <section>
            <div class="center">
                <img height="180px" src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161474/apache.png">
                <div class="fragment">
                    <div style="width:49%;display: inline-block;">
                        <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161546/hdfs.png">
                    </div>
                    <div style="width:49%;display: inline-block;">
                        <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161548/Mapreduce.png">
                    </div>
                </div>
                <div class="fragment">
                    <div style="width:32.9%;display: inline-block;">
                        <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161329/hive3.png">
                    </div>
                    <div style="width:32.9%;display: inline-block;text-align: center;">
                        <img src="images/pig.png">
                        <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1158543/storm.png">
                    </div>
                    <div style="width:32.9%;display: inline-block;">
                        <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1158538/hbase.png">
                    </div>
                </div>
            </div>

        </section>
        <section>
            <h3>MapReduce</h3>
            <div class="center">
                <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165414/MRSandwich.png">
            </div>
            <aside>
                Slide mapreduce : Appuyé sur le fait qu’on peut paralléliser le processus de découpe et d’assemblage indépendamment.
            </aside>
        </section>
        <section>
            <h3>MapReduce</h3>
            <div class="center">
                <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161437/example-mapreduce-wordcount.png">
            </div>
        </section>
        <section>
            <div class="center">
                <p>Beaucoup de lecture/écriture sur disque&nbsp;</p>
                <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161408/disque-dur.png">
            </div>
        </section>
        <section>
            <p>Très verbeux...</p>
            <pre class="java"><code class="hljs">
package org.myorg;

import java.io.IOException;
import java.util.*;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class WordCount {

    public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            StringTokenizer tokenizer = new StringTokenizer(line);
            while (tokenizer.hasMoreTokens()) {
                word.set(tokenizer.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable> {

        public void reduce(Text key, Iterable&lt;IntWritable> values, Context context)
            throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            context.write(key, new IntWritable(sum));
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();

        Job job = new Job(conf, "wordcount");

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);

        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.waitForCompletion(true);
    }

}
            </code></pre>
        </section>
        <section>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161401/spark_logo.png">
        </section>
        <section data-background-image="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165451/baby.jpg">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1158822/amplab_hires.png" height="78px">
        </section>
        <section data-background-image="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165447/renaud.jpg">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161563/apache.png" style="margin-left:13em">
        </section>
        <section data-background-image="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165444/success.jpg">
            <img src="images/databricks.png" height="98px">
        </section>
        <section>
            <div class="center"><img src="images/RAM.png"></div>
        </section>
        <section>
            <div class="center">
                <div style="display: inline-block;width:32.9%">
                    <img width="161px" src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1159009/java.png">
                </div>
                <div style="display: inline-block;width:32.9%">
                    <img width="131px" src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1159016/scala2.png">
                    <br/>
                    <img width="210px" class="fragment" src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1184057/r.png">
                </div>
                <div style="display: inline-block;width:32.9%">
                    <img width="260px" src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1159017/python.png">
                </div>
                <img style="height:225px; display: inline-block;height:50%" src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1158993/spark-stack.png">
            </div>
        </section>
        <section>
            <h3>Resilient Distributed Datasets</h3>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161476/rdd.png">
            <p class="center"><strong>Opérations</strong></p>
            <p><strong>Transformations</strong> (e.g. map, filter, groupBy)</p>

            <p><strong>Actions</strong> (e.g.&nbsp;count, collect)</p>
        </section>
        <section>
            <div class="center">
                <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161567/Information.png">
                <p><span class="bold">Un RDD est immutable</span></p>
            </div>
        </section>
        <section>
            <h3>Quelques exemples</h3>
            <pre class="scala"><code class="hljs">
scala> val textFile = sc.textFile("README.md")
textFile: org.apache.spark.rdd.RDD[String] = README.md MappedRDD[3] at textFile at &lt;console>:12

scala> textFile.count()
res0: Long = 141

scala> textFile.first()
res1: String = # Apache Spark

scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: org.apache.spark.rdd.RDD[String] = FilteredRDD[4] at filter at &lt;console>:14

scala> linesWithSpark.count()
res2: Long = 21

scala> textFile.cache()
res3: textFile.type = README.md MappedRDD[1] at textFile at &lt;console>:12
            </code></pre>
        </section>
        <section>
            <h3>Quelques commandes utiles</h3>
            <pre><code class="hljs scala">
val textFile = sc.textFile("README.md")

// Prendre les 10 premières ligne d'un RDD
val first10 = textFile.take(10)

// Les afficher
first10.foreach(println)

// Les tuples
val tuple = ("key1", 3)
println(tuple._1) // Affiche key1
println(tuple._2) // Affiche 3
            </code></pre>
        </section>
        <section>
            <h3>WordCount</h3>
            <pre class="scala"><code class="hljs">
import org.apache.sparkContext
import org.apache.sparkContext._

object wordCount {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("WordCount").setMaster("local[4]")
    val sc = new SparkContext(conf)
    val textFile = sc.textFile("README.md")
    val count = textFile.flatMap(line => line.split(" "))
        .map(word => (word, 1))
        .reduceByKey(_+_)

    count.saveAsTextFile("/path/to/file")
  }
}

            </code></pre>
        </section>
        <section>
            <h3>Mise en pratique</h3>
            // TODO
        </section>
        <section data-background-image="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165437/machine-learning.jpg">
            <h2 style="text-align: right">MLlib</h2>
        </section>
        <section>
            <ul>
                <li>Kmeans</li>
                <li>Naive Bayes</li>
                <li>Random Forests</li>
                <li>Regressions (logistic&nbsp;&amp;&nbsp;linear)</li>
                <li>Support Vector Machine</li>
                <li>Decision Trees</li>
                <li>Gradient Boosting</li>
                <li>Alternative Least Square</li>
                <li>Principal Component Analysis</li>
                <li>Stochastic Gradient Descent</li>
            </ul>
        </section>
        <section>
            <h3>Les Vectors</h3>
            <pre class="scala"><code class="hljs">
import org.apache.spark.mllib.linalg.{Vector, Vectors}

// Create a dense vector (1.0, 0.0, 3.0).
val dv = Vectors.dense(1.0, 0.0, 3.0)

// Create a sparse vector (1.0, 0.0, 3.0).
val sv1 = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))

// Create a sparse vector (1.0, 0.0, 3.0).
val sv2 = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))
            </code></pre>

        </section>
        <section>
            <h3>Les Labeled Point</h3>
            <pre class="scala"><code class="hljs">
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

// Create a labeled point with a positive label and a dense feature vector.
val pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))

// Create a labeled point with a negative label and a sparse feature vector.
val neg = LabeledPoint(0.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0)))
            </code></pre>
        </section>
        <section>
            <h2 style="text-align: right">Le Naïve Bayes en Spark</h2>
        </section>
        <section>
            <h3>Le Naïve Bayes en Spark</h3>
            <section>
                <pre class="scala"><code class="hljs">
    import org.apache.spark.mllib.linalg.Vectors
    import org.apache.spark.mllib.regression.LabeledPoint
    import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}

    val data = sc.textFile("data/mllib/sample_naive_bayes_data.txt")

    val parsedData = data.map { line =>
    val parts = line.split(',')
    LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
    }
                </code></pre>
            </section>
            <section>
                <pre class="scala"><code class=" hljs ">
    // Split data into training (60%) and test (40%).
    val Array(training, test) = parsedData.randomSplit(Array(0.6, 0.4), seed = 11L)

    // Model construction
    val model = NaiveBayes.train(training, lambda = 1.0)

    // Prediction
    val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))
    val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()

    // Save and load model
    model.save(sc, "myModelPath")
    val sameModel = NaiveBayesModel.load(sc, "myModelPath")
                </code></pre>
            </section>
            <section>
                <h2>Implémentation</h2>
            </section>
            <section>
                <h4>Présentation des packages</h4>
                <div class="half">
                    <img src="images/organisation_projet.png">
                </div>
                <div class="half">
                    <span class="bold">Deux packages:</span>
                    <li>Solution</li>
                    <li>Stubs -> Le package sur lequel travailler</li>

                    <span class="bold">Composition des packages:</span>
                    <li>features : Package pour le feature engineering. Préparation des features et création de nouvelles</li>
                    <li>tools : Fonctions permettant le calcul de métriques ainsi que la transformation de certains rdds</li>
                    <li>modelling (pour la partie Random Forest) : Fonctions relatives à la construction du modèle de prédiction</li>
                    <li>main</li>
                </div>
            </section>
            <section id="data_science_pipeline">
                <h4>Démarche globale en Data Science</h4>
                <p class="offset-left">
                    <li>
                        Step 1 : Chargement des données
                    </li>
                    <li>
                        Step 2 : Feature Engineeging (Nettoyage, création et préparation des features)
                    </li>
                    <li>
                        Step 3 : Splitting (train, validation et test sets)
                    </li>
                    <li>
                        Step 4 : Tuning des paramètres (à la main ou grid search)
                    </li>
                    <li>
                        Step 5 : Modelling sur train+validation set
                    </li>
                    <li>
                        Step 6 : Prediction sur le test set
                    </li>
                </p>
            </section>
            <section>
                <h4>Solution : Main</h4>
                <pre class="scala"><code class=" hljs ">
// Loading Data
val data = sc.textFile("./source/sms_train.csv")

// Parsing & Feature Engineering
val dataParsed = featureEngineering(data)

// Splitting
val Array(trainSet, testSet) = dataParsed.randomSplit(Array(0.75, 0.25))
trainSet.cache()

// Modelling
val model = NaiveBayes.train(trainSet, lambda = 1.0)

// Evaluation
val trainMetrics: MulticlassMetrics = getMetrics(model, trainSet)
val testMetrics: MulticlassMetrics = getMetrics(model, testSet)

val accuracyTrain = trainMetrics.precision
val confusionTrain = trainMetrics.confusionMatrix
val accuracyTest = testMetrics.precision
val confusionTest = testMetrics.confusionMatrix

// Print results
println(s"Train Error: $accuracyTrain")
println(s"Confusion Matrix on training set: \n $confusionTrain")
println(s"Test Error: $accuracyTest")
println(s"Confusion Matrix on test set: \n $confusionTest")
                </code></pre>
            </section>
            <section>
                <h4>Solution : Feature Engineering</h4>
                <pre class="scala"><code class=" hljs ">
val SPAM = "spam"

def featureEngineering(data : RDD[String]): RDD[LabeledPoint] = {

    val targets = data.map(line => if (line.split("\t")(0) == SPAM) 1.0 else 0.0)

    // RDD of words in sms
    val smsRDD: RDD[Seq[String]] = data.map(line => line.split("\t")(1))
      .map(_.split(" ").toSeq)

    // HashingTF
    val hashingTF = new HashingTF()
    val tf: RDD[Vector] = hashingTF.transform(smsRDD)
    tf.cache()

    // TF-IDF
    val idf = new IDF().fit(tf)
    val tfidf: RDD[Vector] = idf.transform(tf)

    // Zip targets and features and convert to LabeledPoint
    targets.zip(tfidf).map(l => LabeledPoint(l._1, l._2))
}
                </code></pre>
            </section>
            <section>
                <h4>Solution : Metrics</h4>
                <pre class="scala"><code class=" hljs ">
def getMetrics(model: NaiveBayesModel, data: RDD[LabeledPoint]): MulticlassMetrics = {

    val predictionsAndLabels = data.map(example => (model.predict(example.features), example.label))

    new MulticlassMetrics(predictionsAndLabels)
}
                </code></pre>
            </section>
        </section>
        <section>
            <h2 style="text-align: right">Random Forest en Spark</h2>
        </section>
        <section>
            <h3>Arbres de décision</h3>
            <section>
                <div class="half">
                    <img src="images/tableau.png">
                </div>
                <div class="half">
                    <img src="images/DecisionTree.png">
                </div>
                <div>
                    <p>
                        <span class="bold">Règles de Décision :</span>
                    </p>
                    <p>
                        <span class="bold">If</span> Outlook=Sunny <span class="bold">and</span> Humidity=Normal <span class="bold">then</span> Yes
                    </p>
                    <p>
                        <span class="bold">If</span> Outlook=Sunny <span class="bold">and</span> Humidity=High <span class="bold">then</span> No
                    </p>
                    <p>
                        <span class="bold">If</span> Outlook=Rainy <span class="bold">and</span> Windy=True <span class="bold">then</span> No
                    </p>
                    <p>
                        <span class="bold">If</span> Outlook=Rainy <span class="bold">and</span> Windy=False <span class="bold">then</span> Yes
                    </p>
                </div>
            </section>
            <section>
                <h4>Mesure de l'incertitude: Entropie</h4>
                <p><span class="bold">Entropie :</span> Permet de quantifier le degré d'incertitude sur une variable. Mesurée en bits.</p>
                $$H(X) = - \sum_{k=1}^{K}{P(x_k) log(P(x_k))}$$
                <p><span class="bold">Exemple :</span> X = Résultat d'un lancer de pièce.</p>
                $$H(X) = - [P(X=Pile) log(P(X=Pile)) + P(X=Face) log(P(X=Face))]$$
                <div>
                    <div>
                        Si pièce non truquée:
                        $$P(x=Pile) = P(X=Face) = 0.5$$
                        $$H(X) = -(0.5*log(0.5) + 0.5*log(0.5)) = 1 bit$$
                    </div>
                    <div>
                        Si pièce truquée:
                        $$P(x=Pile) = 0.8; P(X=Face) = 0.2$$
                        $$H(X) = -(0.8*log(0.8) + 0.2*log(0.2)) = 0.722 bit$$
                    </div>
                </div>
            </section>
            <section>
                <h4>Gain en information</h4>
                <div class="center"><img src="images/arbre.png" height="200px"></div>
                <p>Comment mesurer la réduction d'incertitude ?</p>
                <p><span class="bold">Information Gain</span> $$IG = H_0 - (\frac{|T_{1,1}|}{|T_0|}H_{1,1} + \frac{|T_{1,2}|}{|T_0|}H_{1,2})$$</p>
                <p class="center"><span class="bold">A chaque split, on teste toutes les features encore à disposition, et on sélectionne la feature permettant le gain en information le plus important</span></p>
            </section>
            <section>
                <h4>Exemple</h4>
                <div class="half">
                    <img src="images/outlook.png">
                </div>
                <div class="half">
                    <p>
                        $$H_{0} = - \frac{5}{10}log(\frac{5}{10}) - \frac{5}{10}log(\frac{5}{10}) = 1$$
                    </p>
                    <p>
                        $$H_{1,1} = - \frac{2}{5}log(\frac{2}{5}) - \frac{3}{5}log(\frac{3}{5}) = 0.97$$
                    </p>
                    <p>
                        $$H_{1,2} = - \frac{2}{5}log(\frac{2}{5}) - \frac{3}{5}log(\frac{3}{5}) = 0.97$$
                    </p>
                    <p>
                        $$IG_{Outlook} = H_{0} -  (\frac{5}{10} * H_{1,1} + \frac{5}{10} * H_{1,2}) = 0.03$$
                    </p>
                </div>
                <div class="center">
                    <img src="images/tableau4.png" style="width: 40%; height: 40%" class="fragment">
                </div>
            </section>
        </section>
        <section>
            <h3>Random Forest</h3>
            <section>
                <div class="half">
                    <img src="images/RandomForest.png">
                </div>
                <div class="half">
                    <p><span class="bold">Idée : </span>Créer plusieurs arbres et rassembler les prédictions pour prendre une décision finale plus stable.</p>
                    <p><span class="bold">Principe:</span>
                        <ul>
                            <li>Ajout d'aléatoire dans la sélection des données</li>
                            <li>Ajout d'aléatoire dans la sélection des features</li>
                        </ul>
                    </p>
                    <p><span class="bold">Conclusion :</span> Moins interprétable (perte de la structure hiérarchique) mais gain en stabilité et robustesse</p>
                </div>
            </section>
        </section>
        <section>
            <h3>Mise en pratique</h3>
            <section id="bike-demand-prediction">
                <h4>Prédiction de la demande de vélos</h4>
                <p>
                    Features présentes dans le dataset bike_train.csv
                    <li>
                        <span class="bold">date</span> : La date au format "YYYY-MM-DD HH:MM:SS"
                    </li>
                    <li>
                        <span class="bold">season_{1,2,3,4}</span> : Saison (binaire)
                    </li>
                    <li>
                        <span class="bold">holiday</span> : Vacances (binaire)
                    </li>
                    <li>
                        <span class="bold">weather_{1,2,3}</span> : Catégorie de temps (binaire)
                    </li>
                    <li>
                        <span class="bold">temperature</span> : Température moyenne (entre 0 et 1) 
                    </li>
                    <li>
                        <span class="bold">humidity</span> : Humidité moyenne (entre 0 et 1)
                    </li>
                    <li>
                        <span class="bold">windspeed</span> : Vitesse moyenne du vent
                    </li>
                    <li>
                        <span class="bold">count</span> : La variable cible - Nombre de vélos loués pendant la journée
                    </li>
                </p>
                <p>
                    Métrique utilisée : <span class="bold">Root Mean Square Error</span>
                    $$RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2}$$
                </p>
            </section>
            <section>
                <h4>Les Data Frames</h4>
                <pre class="scala"><code class="hljs">
    // sc is an existing SparkContext.
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    // this is used to implicitly convert an RDD to a DataFrame.
    import sqlContext.implicits._

    // Define the schema using a case class.
    case class Person(name: String, age: Int)

    // Create an RDD of Person objects and register it as a table.
    val people = sc.textFile("examples/src/main/resources/people.txt")
                    .map(_.split(","))
                    .map(p => Person(p(0), p(1).trim.toInt))
                    .toDF()
    people.registerTempTable("people")

    // Examples
    df.select("name").show()
    df.filter(df("name") > 21).show()
    df.groupBy("age").count().show()
                </code></pre>
            </section>
            <section id="data_frame_exploration">
                <h4>Exploration des données</h4>
                <p class="offset-left">
                    <li>
                        Dans le shell, chargez le dataset dans un RDD, puis transformez-le en DataFrame
                    </li>
                    <li>
                        Observez quelques valeurs de chaque feature
                    </li>
                    <li>
                        Calculez quelques statistiques que chaque feature pour voir s'il n'y a pas de données manquantes ou d'outliers
                    </li>
                    <li>
                        Modifiez les données si besoin
                    </li>
                </p>
            </section>
            <section>
                <h2>Implémentation</h2>
            </section>
            <section>
                <h4>Etapes d'implémentation</h4>
                <p><span class="bold">TODO 1</span> : Préparer les données (sauf les dates) et implémenter un Random Forest sur des paramètres choisis à la main</p>
                <p><span class="bold">TODO 2</span> : Créer de nouvelles features provenant de la date et observer l'amélioration des résultats de prédicion (baisse du RMSE)</p>
                <p><span class="bold">TODO 3</span> : Implémenter un Grid Search pour trouver les meilleurs paramètres du Random Forest et observer l'amélioration des résultats de prédiction</p>
            </section>
            <section>
                <h4>Solution TODO 1: Main</h4>
                <pre class="scala"><code class="hljs">
// Loading Data
val data = sc.textFile("./source/bike_train.csv")

// Parsing Data & Feature Engineering
val schemaData = extractHeader(data)
val dataParsed = featureEngineering(schemaData._2)

// Splitting
val Array(trainSet, valSet, testSet) = dataParsed.randomSplit(Array(0.8, 0.1, 0.1))
trainSet.cache()
valSet.cache()

// Modelling
val dataTrain = sc.union(trainSet, valSet)
val categoricalFeaturesInfo = Map(0 -> 4, 3 -> 4)
val model = randomForestTrainRegressor(categoricalFeaturesInfo=categoricalFeaturesInfo,
  numTrees = 10, maxDepth = 20, maxBins = 30)(dataTrain)

// Evaluation
val rmseTrain = getRMSE(model, dataTrain)
val rmseTest = getRMSE(model, testSet)

// Show Evaluation results
println(s"Train Error: $rmseTrain")
println(s"Test Error: $rmseTest")
                </code></pre>
            </section>
            <section>
                <h4>Solution TODO 1: Feature Engineering</h4>
                <pre class="scala"><code class="hljs">
def featureEngineering(rdd: RDD[String]): RDD[LabeledPoint] = {

    val dataParsed = rdd.map {
      line =>

        val values = line.split(',')

        // Convert all features but date to double
        val valuesNoDate = values.slice(1,values.size).map(_.toDouble)

        // Convert all season categories into one categorical feature
        val season = valuesNoDate.slice(0, 4).indexOf(1.0).toDouble

        // Convert all weather categories into one categorical feature
        val weather = valuesNoDate.slice(6, 10).indexOf(1.0).toDouble

        // Put all final features into a dense Vector
        val array = Array(season) ++ valuesNoDate.slice(4,6) ++
          Array(weather) ++ valuesNoDate.slice(10, valuesNoDate.size-1)
        val featureVector = Vectors.dense(array)
        val label = valuesNoDate.last

        LabeledPoint(label, featureVector)
    }
    dataParsed
}
                </code></pre>
            </section>
            <section>
                <h4>Solution TODO 1: Metrics</h4>
                <pre class="scala"><code class="hljs">
def getRMSE(model: RandomForestModel, data: RDD[LabeledPoint]): Double = {

    val predictionsAndLabels = data.map(example => (model.predict(example.features), example.label))
    calculateRMSE(predictionsAndLabels)
}

def calculateRMSE(rdd: RDD[(Double, Double)]): Double ={
    math.sqrt(rdd.map{case(v,p) => math.pow(v - p, 2)}.mean())
}
                </code></pre>
            </section>
            <section id="date_features">
                <h4>TODO 2 : Date features</h4>
                <p class="offset-left">
                    <p>
                        <span class="bold">Réalisation :</span>
                        Créer de nouvelles features provenant de la date et observer l'amélioration des résultats de prédicion (baisse du RMSE)
                    </p>
                </p>
            </section>
            <section>
                <h4>Solution TODO 2: Feature Engineering</h4>
                <pre class="scala"><code class="hljs">
def featureEngineering(rdd: RDD[String]): RDD[LabeledPoint] = {

    val dataParsed = rdd.map {
      line =>

        val values = line.split(',')

        // Create features from dates
        val dateString = values(0).split(" ")(0)
        val date = new DateTime(dateString)

        val dayOfWeek = date.dayOfWeek().get() - 1
        val month = date.monthOfYear().get() - 1
        val year = date.year().get()

        // Convert all features but date to double
        val valuesNoDate = values.slice(1,values.size).map(_.toDouble)

        // Convert all season categories into one categorical feature
        val season = valuesNoDate.slice(0, 4).indexOf(1.0).toDouble

        // Convert all weather categories into one categorical feature
        val weather = valuesNoDate.slice(6, 10).indexOf(1.0).toDouble

        // Put all final features into a dense Vector
        val array = Array(dayOfWeek, month, year, season) ++ valuesNoDate.slice(4,6) ++
          Array(weather) ++ valuesNoDate.slice(10, valuesNoDate.size-1)
        val featureVector = Vectors.dense(array)
        val label = valuesNoDate.last

        LabeledPoint(label, featureVector)
    }
    dataParsed
}
                </code></pre>
            </section>
            <section>
                <h4>Solution TODO 2: Main</h4>
                <pre class="scala"><code class="hljs">
// Loading Data
val data = sc.textFile("./source/bike_train.csv")

// Parsing Data & Feature Engineering
val schemaData = extractHeader(data)
val dataParsed = featureEngineering(schemaData._2)

// Splitting
val Array(trainSet, valSet, testSet) = dataParsed.randomSplit(Array(0.8, 0.1, 0.1))
trainSet.cache()
valSet.cache()

// Modelling
val dataTrain = sc.union(trainSet, valSet)
val categoricalFeaturesInfo = Map(0 -> 7, 1 -> 12, 3 -> 4, 6 -> 4)
val model = randomForestTrainRegressor(categoricalFeaturesInfo=categoricalFeaturesInfo,
  numTrees = 10, maxDepth = 20, maxBins = 30)(dataTrain)

// Evaluation
val rmseTrain = getRMSE(model, dataTrain)
val rmseTest = getRMSE(model, testSet)

// Show Evaluation results
println(s"Train Error: $rmseTrain")
println(s"Test Error: $rmseTest")
                </code></pre>
            </section>
            <section id="grid_search">
                <h4>TODO 3 : Grid Search</h4>
                <p class="offset-left">
                    <p>
                        <span class="bold">Objectif</span> : Tester, pour chaque paramètre, plusieurs possibilités
                    </p>
                    <p>
                        <span class="bold">Réalisation :</span>
                        <li>
                            Pour chaque combinaison de paramètres, construire un modèle basé sur le train set
                        </li>
                        <li> 
                            Calculer le score de prédiction de chaque modèle sur le validation set
                        </li>
                        <li>
                            Sélectionner le jeu de paramètres ayant donné le meilleur score
                        </li>
                    </p>
                </p>
            </section>
            <section>
                <h4>Solution TODO 3 : Modelling</h4>
                <pre class="scala"><code class="hljs">
def gridSearchRandomForestRegressor(trainSet: RDD[LabeledPoint],
                                      valSet: RDD[LabeledPoint],
                                      categoricalFeaturesInfo: Map[Int, Int] = Map[Int, Int](),
                                      numTreesGrid: Array[Int] = Array(10),
                                      featuresSubsetStrategy: String = "auto",
                                      impurity: String = "variance",
                                      maxDepthGrid: Array[Int] = Array(2),
                                      maxBinsGrid: Array[Int] = Array(4)) = {

    val gridSearch =

      for (numTrees <- numTreesGrid;
           maxDepth <- maxDepthGrid;
           maxBins <- maxBinsGrid)
        yield {

          val model = RandomForest.trainRegressor(trainSet, categoricalFeaturesInfo,
            numTrees, featuresSubsetStrategy, impurity, maxDepth, maxBins)

          val rmseVal = getRMSE(model, valSet)

          ((numTrees, maxDepth, maxBins), rmseVal)
        }

    val params = gridSearch.sortBy(_._2).take(1)(0)._1
    val numTrees = params._1
    val maxDepth = params._2
    val maxBins = params._3

    (categoricalFeaturesInfo, numTrees, featuresSubsetStrategy, impurity, maxDepth, maxBins)
}
                </code></pre>
            </section>
            <section>
                <h4>Solution TODO 3 : Main</h4>
                <pre class="scala"><code class="hljs">
// Loading Data
val data = sc.textFile("./source/bike_train.csv")

// Parsing Data & Feature Engineering
val schemaData = extractHeader(data)
val dataParsed = featureEngineering(schemaData._2)

// Splitting
val Array(trainSet, valSet, testSet) = dataParsed.randomSplit(Array(0.8, 0.1, 0.1))
trainSet.cache()
valSet.cache()

// Model tuning
val categoricalFeaturesInfo = Map(0 -> 7, 1 -> 12, 3 -> 4, 6 -> 4)
val numTreesGrid = Array(500, 1000)
val maxDepthGrid = Array(20, 25, 30)
val maxBinsGrid = Array(50, 100, 200)
val bestParams = gridSearchRandomForestRegressor(trainSet, valSet,
  categoricalFeaturesInfo = categoricalFeaturesInfo, maxDepthGrid = maxDepthGrid,
  maxBinsGrid = maxBinsGrid, numTreesGrid = numTreesGrid)

// Modelling
val dataTrain = sc.union(trainSet, valSet)
val model = (randomForestTrainRegressor _).tupled(bestParams)(dataTrain)

// Evaluation
val rmseTrain = getRMSE(model, dataTrain)
val rmseTest = getRMSE(model, testSet)


// Show Evaluation results
println(s"Best Parameters: ${bestParams}")
println(s"Train Error: $rmseTrain")
println(s"Test Error: $rmseTest")
                </code></pre>
            </section>
        </section>
        <section class="back-page" data-background-image="images/bg.png">
            <h1>Merci</h1>
            <aside class="notes">
                <!-- Put our comments her -->
            </aside>
        </section>
        <section>
            <h3>Mise en pratique</h3>
            <section id="bike-demand-prediction">
                <h4>Prédiction de la demande de vélos</h4>
                Features présentes dans le dataset bike_train.csv
                <p class="offset-left">
                    <p>
                        <span class="bold">date</span> : La date au format "YYYY-MM-DD HH:MM:SS"
                    </p>
                    <p>
                        <span class="bold">season_{1,2,3,4}</span> : Saison (binaire)
                    </p>
                    <p>
                        <span class="bold">holiday</span> : Vacances (binaire)
                    </p>
                    <p>
                        <span class="bold">weather_{1,2,3,4}</span> : Catégorie de temps (binaire)
                    </p>
                    <p>
                        <span class="bold">temperature</span> : Température moyenne (entre 0 et 1) 
                    </p>
                    <p>
                        <span class="bold">humidity</span> : Humidité moyenne (entre 0 et 1)
                    </p>
                    <p>
                        <span class="bold">windspeed</span> : Vitesse moyenne du vent
                    </p>
                    <p>
                        <span class="bold">count</span> : La variable cible - Nombre de vélos loués pendant la journée
                    </p>
                </p>
            </section>
            <section id="data_science_pipeline">
                <h4>Démarche globale en Data Science</h4>
                <p class="offset-left">
                    <p>
                        Step 1 : Chargement des données
                    </p>
                    <p>
                        Step 2 : Feature Engineeging (Nettoyage, création et préparation des features)
                    </p>
                    <p>
                        Step 3 : Splitting (train, validation et test sets)
                    </p>
                    <p>
                        Step 4 : Tuning des paramètres (à la main ou grid search)
                    </p>
                    <p>
                        Step 5 : Modelling sur train+validation set
                    </p>
                    <p>
                        Step 6 : Prediction sur le test set
                    </p>
                </p>
            </section>
            <section id="grid_search">
                <h4>Le Grid Search</h4>
                <p class="offset-left">
                    <p>
                        <span class="bold">Objectif</span> : Tester, pour chaque paramètre, plusieurs possibilités
                    </p>
                    <p>
                        <span class="bold">Réalisation</span>
                        <p>
                            Pour chaque combinaison de paramètres, construire un modèle basé sur le train set
                        </p>
                        <p> 
                            Calculer le score de prédiction de chaque modèle sur le validation set
                        </p>
                        <p>
                            Sélectionner le jeu de paramètres ayant donné le meilleur score
                        </p>
                    </p>
                </p>
            </section>
        </section>
    </div>
    <div class="footer">
        <span class="hashtag">#LearnMLWithSpark</span>
        <span class="twitter">@MatBreton</span>
        <span class="twitter">@YoannBENOIT</span>
        <span class="twitter">@AlbanPhelip</span>
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        backgroundTransition:'slide',
        transition: 'slide', // none/fade/slide/convex/concave/zoom,

        math: {
            mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // Optional reveal.js plugins
        dependencies: [
            { src: 'plugin/math/math.js', async: true },
            {
                src: 'lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'plugin/zoom-js/zoom.js', async: true},
            {src: 'plugin/notes/notes.js', async: true}
        ]
    });

    Reveal.addEventListener('slidechanged', function (event) {
        if (event.indexh == 0) {
            document.querySelector('.reveal').classList.add('slide0');
        } else {
            document.querySelector('.reveal').classList.remove('slide0');
        }

    });


    if (Reveal.getState().indexh == 0) {
        document.querySelector('.reveal').classList.add('slide0');
    }

</script>

</body>
</html>
