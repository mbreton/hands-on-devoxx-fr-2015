<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Initiation au Machine Learning avec Spark</title>


    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">

    <link rel="stylesheet" type="text/css" href="css/theme/devoxx.css" id="theme">


    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/solarized_light.css">

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

    <style>
        pre.small-code {
            font-size: 0.36em;
        }
    </style>
</head>

<body>

<div class="reveal convex center">
    <div class="slides">
        <section>
            <h1>Initiation au Machine Learning avec Spark</h1>
            <aside class="notes">
                <!-- Put our comments her -->
            </aside>
        </section>
        <section>
            <h2>Agenda</h2>
            <ol>
                <li>Les bases du machine learning</li>
                <li>TP Kmeans</li>
                <li>TP Naive Bayes</li>
                <li>- Break -</li>
                <li>Présentation de Spark & MLlib</li>
                <li>Naivge Bayes avec Spark</li>
                <li>Random Forest avec Spark</li>
            </ol>
            <aside class="notes">
                <!-- Put our comments her -->
            </aside>
        </section>
        <section>
            <h2 class="alone">Les bases du machine learning</h2>
            <aside class="notes">
                <!-- Put our comments her -->
            </aside>
        </section>
        <section>
            <section>
                <h3>Qu'est ce donc ?</h3>
                <blockquote>
                    L’apprentissage machine est l’étude des algorithmes
                    qui s’améliorent automatiquement en fonction de l’expérience.
                </blockquote>
                -- Tom Mitchell
                <aside class="notes">
                    <!-- Put our comments her -->
                </aside>
            </section>
            <section>
                On utilise leurs résultats sans forécement s'en rendre compte
                <aside class="notes">
                    <!-- Put our comments her -->
                </aside>
            </section>
        </section>
        <section>
            <h3>Cas d'utilisation</h3>
            <p>Classification</p>
        </section>
        <section>
            <h3>Cas d'utilisation</h3>
            <div>
                <p>Régréssion</p>
            </div>
            <div>
                <p>Graph</p>
            </div>
        </section>
        <section>
            <h3>Cas d'utilisation</h3>
            <div>
                <p>Recommandation</p>
                <img src="images/reco.jpg" >
            </div>
            <div>
                <p>Graph</p>
            </div>
        </section>
        <section>
            <h3>Cas d'utilisation</h3>
            <div>
                <p>Clustering</p>
                <img src="images/reco.jpg" >
            </div>
            <div>
                <p>Clustering</p>
            </div>
        </section>
        <section>
            <h3>Approches : supervisé (dirigé)</h3>
               On dispose d'exemples, et pour chaque exemple, le résultat attendu.
            <p>
                Ex: Des tweets et leurs tags associés
            </p>
        </section>
        <section>
            <h3>Approches : non supervisé</h3>
            On dispose de données brutes, mais pas de résultat attendu. Juste des données brutes.
            <p>
                Ex: Logs server web
            </p>
        </section>
        <section>
            <h3>Approches : apprentissage par renforcement</h3>
            Pour chaque état dans notre domaine, on peut associer une récompense.
            <p>
                Ex: Vol acrobatique d'hélicoptère
            </p>
            <a href="http://cs.stanford.edu/groups/helicopter/papers/nips06-aerobatichelicopter.pdf">cs.stanford.edu/groups/helicopter/papers/nips06-aerobatichelicopter.pdf</a>
        </section>
        <section>
            <h3>Comment en arrive-t'on à avoir besoin de ML ?</h3>
            <ol>
                <li>On observe un phénomène</li>
                <li>On a du mal à trouver un algo précis</li>
                <li>On utilise un algo qui va apprendre à approximer le phénomène réel</li>
            </ol>
        </section>
        <section>
            <h3>A l'utilisation, comment ça se passe ?</h3>
            Le machine Machine learning c'est comme la crypto
            <p>Utilisez des implémentations éprouvées.</p>
        </section>
        <section>
            <h2>Mais à quoi "ça" ressemble ?</h2>
        </section>
        <section>
            <h3>Kmeans</h3>
            <section>
                <p>Type: Non supervisé</p>
                <p>But: Trouver des groupes dans un jeu de données</p>
            </section>
            <section>
                <p>Exemple : Quels groupes puis-je identifier dans ma base client ?</p>
            </section>
            <section>
                <ul>
                    <li>Client 1 : 24ans, 1 exemplaire</li>
                    <li>Client 2 : 32ans, 2 exemplaires</li>
                    <li>Client 3 : 27ans, 5 exemplaires</li>
                </ul>
            </section>
            <section>
                (24;1), (32;2), (37;5)
            </section>
            <section>
                <img src="images/kmean1.png">
            </section>
            <section>
                <img src="images/kmean2.png">
            </section>
            <section>
                <img src="images/kmean3.png">
            </section>
            <section>
                <img src="images/kmean4.png">
            </section>
            <section>
                <img src="images/kmean5.png">
            </section>
            <section>
                <img src="images/kmean6.png">
            </section>
        </section>
        <section>
            <h2>Classification naive Bayesienne</h2>
        </section>
        <section>
            <h3>Classification naive Bayesienne</h3>
            <section>
                <p>Type: Supervisé</p>
                <p>But: Déterminer la catégorie d'un élément</p>
            </section>
            <section>
                <p>Probabilités</p>
                <p>&Omega;, F, P</p>
            </section>
            <section>
                <p>$$p(A|B) = \frac{p(B|A) p(A)}{p(B)}$$</p>
            </section>
            <section>
                <p>M : J'ai un message M</p>
                <p>S : C'est un spam</p>
                <p>$$p(S|M) = \frac{p(M|S) p(S)}{p(M)}$$</p>
            </section>
            <section>
                <p>$$p(S|W_1,...,W_n) = \frac{p(W_1,...,W_n|S) p(S)}{p(W_1,...,W_n)}$$</p>
            </section>
            <section>
                $$k=\frac{p(S=1|W_1,...,W_n)}{p(S=0|W_1,...,W_n)}= \frac{p(W_1,...,W_n|S=1)* p(S=1)}{p(W_1,...,W_n)}*\frac{p(W_1,...,W_n)}{p(W_1,...,W_n|S=0)* p(S=0)}$$
            </section>
            <section>
                <p>Hypothèse dans le cas naif : indépendance</p>
                $$p(A,B|C)=p(A|C)p(B|C)$$
                $$p(W_1,...W_n|S=1)=p(W_1|S=1) p(W_2|S=1)...(W_n|S=1)$$
            </section>
            <section>
                $$p(W_1,...W_n|S)=\prod_{i=1}^{N}{p(W_i|S)}$$
                $$k=\frac{\prod_{i=1}^{N}{p(W_i|S=1)p(S=1)}}{\prod_{i=1}^{N}{p(W_i|S=0) * p(S=0)}}$$
            </section>
            <section>
                <h4>Du coup ?</h4>
                $$k=\frac{\prod_{i=1}^{N}{p(W_i|S=1)p(S=1)}}{\prod_{i=1}^{N}{p(W_i|S=0) * p(S=0)}}$$
                $$p(S=1)=\frac{\text{nombre de spams observés}}{\text{nombre total de messages}}$$
                $$p(S=0) = 1 - p(S=1)$$
                $$p(W_i|S=t)=\frac{\text{nombre de messages de type t dans lesquels W_i apparait}}{\text{nombre de messages de type t}}$$
            </section>
            <section>
                <h2>Implémentation</h2>
            </section>
            <section>
                <h3>Un peu de recul sur l'algo</h3>
                Est-ce un bon score ? (> 0.5 et |spam|/|total| ?)<br/>
                Est-ce que mes données d'entraînement sont suffisament représentatives ?<br/>
                Est-ce que mes résultats se généralisent<br/>
                L'approche naïve est elle pertinente ?
                <aside class="notes">
                    données représentatives: est-ce que le ratio nb spam/nb ham est représentatif de ce que l’on retrouve dans la nature ? (vu le point de ce rapport)
                </aside>
            </section>
        </section>
        <section>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161394/logo_hadoop.png">
        </section>
        <section>
            <h3>MapReduce</h3>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1158538/hbase.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1158543/storm.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161329/hive3.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161474/apache.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161546/hdfs.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161548/Mapreduce.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165414/MRSandwich.png">
        </section>
        <section>
            <h3>MapReduce</h3>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161437/example-mapreduce-wordcount.png">
        </section>
        <section>
            <p>Beaucoup de lecture/écriture sur disque&nbsp;</p>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161408/disque-dur.png">
        </section>
        <section>
            <p>Très verbeux...</p>
            <pre class="java"><code class="hljs">
package org.myorg;

import java.io.IOException;
import java.util.*;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class WordCount {

    public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            StringTokenizer tokenizer = new StringTokenizer(line);
            while (tokenizer.hasMoreTokens()) {
                word.set(tokenizer.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable> {

        public void reduce(Text key, Iterable&lt;IntWritable> values, Context context)
            throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            context.write(key, new IntWritable(sum));
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();

        Job job = new Job(conf, "wordcount");

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);

        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.waitForCompletion(true);
    }

}
            </code></pre>
        </section>
        <section>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161401/spark_logo.png">
        </section>
        <section data-background-image="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165451/baby.jpg">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1158822/amplab_hires.png">
        </section>
        <section data-background-image="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165447/renaud.jpg">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161563/apache.png">
        </section>
        <section data-background-image="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165444/success.jpg">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1158993/spark-stack.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1159009/java.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1159016/scala2.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1159017/python.png">
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1184057/r.png"></section>
        <section>
            <h3>Resilient Distributed Datasets</h3>

            <p><strong>Transformations</strong> (e.g. map, filter, groupBy)</p>

            <p><strong>Actions</strong> (e.g.&nbsp;count, collect)</p>

            <p><strong>Opérations</strong></p>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161476/rdd.png">
        </section>
        <section>
            <h1>Un RDD est immutable</h1>
            <img src="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1161567/Information.png">
        </section>
        <section>
            <h3>Quelques exemples</h3>
            <pre class="scala"><code class="hljs">
scala> val textFile = sc.textFile("README.md")
textFile: org.apache.spark.rdd.RDD[String] = README.md MappedRDD[3] at textFile at &lt;console>:12

scala> textFile.count()
res0: Long = 141

scala> textFile.first()
res1: String = # Apache Spark

scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: org.apache.spark.rdd.RDD[String] = FilteredRDD[4] at filter at &lt;console>:14

scala> linesWithSpark.count()
res2: Long = 21

scala> textFile.cache()
res3: textFile.type = README.md MappedRDD[1] at textFile at &lt;console>:12
            </code></pre>
        </section>
        <section>
            <h3>Quelques commandes utiles</h3>
            <pre><code class="hljs scala">
val textFile = sc.textFile("README.md")

// Prendre les 10 premières ligne d'un RDD
val first10 = textFile.take(10)

// Les afficher
first10.foreach(println)

// Les tuples
val tuple = ("key1", 3)
println(tuple._1) // Affiche key1
println(tuple._2) // Affiche 3
            </code></pre>
        </section>
        <section>
            <h3>MapReduce</h3>
            <pre class="scala"><code class="hljs">
val textFile = sc.textFile("README.md")

val count = textFile.flatMap(line => line.split(" "))
.map(word => (word, 1))
.reduce(_+_)

textFile.saveAsTextFile("/path/to/file")
            </code></pre>
        </section>
        <section
                data-background-image="https://s3.amazonaws.com/media-p.slid.es/uploads/albanphelip/images/1165437/machine-learning.jpg">
            <h3>MLlib</h3>
        </section>
        <section>
            <ul>
                <li>Kmeans</li>
                <li>Naive Bayes</li>
                <li>Random Forests</li>
                <li>Regressions (logistic&nbsp;&amp;&nbsp;linear)</li>
                <li>Support Vector Machine</li>
                <li>Decision Trees</li>
                <li>Gradient Boosting</li>
                <li>Alternative Least Square</li>
                <li>Principal Component Analysis</li>
                <li>Stochastic Gradient Descent</li>
            </ul>
        </section>
        <section>
            <h3>Les Vectors</h3>
            <pre class="scala"><code class="hljs">
import org.apache.spark.mllib.linalg.{Vector, Vectors}

// Create a dense vector (1.0, 0.0, 3.0).
val dv = Vectors.dense(1.0, 0.0, 3.0)

// Create a sparse vector (1.0, 0.0, 3.0).
val sv1 = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))

// Create a sparse vector (1.0, 0.0, 3.0).
val sv2 = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))
            </code></pre>

        </section>
        <section>
            <h3>Les Labeled Point</h3>
            <pre class="scala"><code class="hljs">
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

// Create a labeled point with a positive label and a dense feature vector.
val pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))

// Create a labeled point with a negative label and a sparse feature vector.
val neg = LabeledPoint(0.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0)))
            </code></pre>
        </section>
        <section>
            <h3>Un exemple : le Naïve Bayes</h3>
            <pre class="scala"><code class="hljs">
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}

val data = sc.textFile("data/mllib/sample_naive_bayes_data.txt")

val parsedData = data.map { line =>
val parts = line.split(',')
LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
}
            </code></pre>
        </section>
        <section>
            <h3>Un exemple&nbsp;: le Naïve Bayes</h3>
            <pre class="scala"><code class=" hljs ">
// Split data into training (60%) and test (40%).
val Array(training, test) = parsedData.randomSplit(Array(0.6, 0.4), seed = 11L)

// Model construction
val model = NaiveBayes.train(training, lambda = 1.0)

// Prediction
val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))
val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()

// Save and load model
model.save(sc, "myModelPath")
val sameModel = NaiveBayesModel.load(sc, "myModelPath")
            </code></pre>
        </section>
        <section>
            <h3>Arbres de décision</h3>
            <section>
                <h4>Une démarche courante</h4>
                <div>
                    <p>Algorithme récursif: A chaque étape, on split le jeu
                        de données en se basant sur le test d'une feature.
                    </p>
                    <p>
                        Choix de la feature: Celle qui réduit le plus notre incertitude sur la classification.
                    </p>
                    <p>
                        Exemple: Savoir si un enfant peut oui ou non aller jouer dehors avec son ami.
                    </p>
                </div>
                <div>
                    <img src="images/arbre2.png">
                </div>
            </section>
            <section>
                <h4>Mesre de l'incertitude: Entropie</h4>
                <p>Entropie: Permet de quantifier le degré d'incertitude sur une variable. Mesurée en bits.</p>
                $$H(X) = - \sum_{k=1}^{K}{P(x_k) log(P(x_k))}$$
                <p>Exemple: X = Résultat d'un lancer de pièce.</p>
                $$H(X) = - [P(X=Pile) log(P(X=Pile)) + P(X=Face) log(P(X=Face))]$$
                <div>
                    <div>
                        Si pièce non truquée:
                        $$P(x=Pile) = P(X=Face) = 0.5$$
                        $$H(X) = -(0.5*log(0.5) + 0.5*log(0.5)) = 1 bit$$
                    </div>
                    <div>
                        Si pièce non truquée:
                        $$P(x=Pile) = 0.8; P(X=Face) = 0.2$$
                        $$H(X) = -(0.8*log(0.8) + 0.2*log(0.2)) = 0.722 bit$$
                    </div>
                </div>
            </section>
            <section>
                <h4>Gain en information</h4>
                <img src="images/arbre.png" >
                <p>Comment mesurer la réduction d'incertitude ?</p>
                <p>Information Gain $$IG = H_0 - (\frac{|T_{1,1}|}{|T_0|}H_{1,1} + \frac{|T_{1,2}|}{|T_0|}H_{1,2})$$</p>
                <p>A chaque split, on teste toutes les features encore à disposition, et on sélectionne la feature permettant le gain en information le plus important</p>
            </section>
        </section>
        <section>
            <h3>Random Forest</h3>
            <section>
                <p>Instabilité des arbres de décision: Faible changement dans les données peut entraîner un arbre totalement différent.</p>
                <p>Idée: Créer plusieurs arbres et rassembler les prédictions pour prendre une décision finale plus stable.</p>
                <p>
                    Principe:
                <ul>
                    <li>Chaque arbre est construit à partir de tirages avec replacement de l'ensemble d'apprentissage -> Ajout d'aléatoire dans les données</li>
                    <li>Pour chaque split de chaque arbre: Choix de la meilleure feature à partir d'un sous-ensemble des features à disposition -> Ajout d'aléatoire dans le choix des features</li>
                </ul>
                </p>
                <p>Conclusion: Moins interprétable (perte de la structure hiérarchique) mais gain en stabilité et robustessen</p>
            </section>
            <section>
                <h4>Prédiction</h4>
                <p>Prediction de la classe d'une nouvelle donnée</p>
                Pour chaque arbre, on descend noeud par noeud en suivant les règles apprises, jusqu'à obtenir la classe prédite.<br/>
                Pour B arbres, on a alors:
                <p class="offset-left">$$\tilde{y_b}(x)$$ Prédiction pour l'arbre b</p>
                <p class="offset-left">$$C_k(x)$$ Le nombre d'arbres pour lesquels la classe prédite est k</p>
                <p>$$C_k(x) = \sum_{b=1}^{B}{I(\tilde{y_b}(x) = k)}$$</p>
                <p>La prédiction finale est alors: $$\tilde{y}(x) = argmax_k(C_k(x))$$</p>
            </section>
        </section>
    </div>
    <div class="footer">
        <span class="hashtag">#LearnMLWithSpark</span>
        <span class="twitter">@AlbanPhelip</span>
        <span class="twitter">@YoannBENOIT</span>
        <span class="twitter">@MatBreton</span>
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom,

        math: {
            mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // Optional reveal.js plugins
        dependencies: [
            { src: 'plugin/math/math.js', async: true },
            {
                src: 'lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'plugin/zoom-js/zoom.js', async: true},
            {src: 'plugin/notes/notes.js', async: true}
        ]
    });

    Reveal.addEventListener('slidechanged', function (event) {
        if (event.indexh == 0) {
            document.querySelector('.reveal').classList.add('slide0');
        } else {
            document.querySelector('.reveal').classList.remove('slide0');
        }

    });


    if (Reveal.getState().indexh == 0) {
        document.querySelector('.reveal').classList.add('slide0');
    }

</script>

</body>
</html>
